# -*- coding: utf-8 -*-
"""Desafio Pipeline de ETL Integrado - Semana 15.ipynb

Automatically generated by Colab.

Original file is located at
https://colab.research.google.com/drive/1nJeBiBHJk6FEgh7I_VeQUNZ_g2b4xPoc
"""

import pandas as pd
import requests
import json
import sqlite3
import logging
import os
import yaml
from prefect import task, flow, get_run_logger
import subprocess

print("Ambiente configurado com sucesso!")

# ========= 1.2 - Fonte de Dados SQL =========

conn_bootcamp = sqlite3.connect("bootcampBI.db")
cursor_bootcamp = conn_bootcamp.cursor()

cursor_bootcamp.execute("""
CREATE TABLE IF NOT EXISTS PARTICIPANTES (
    ID_PARTICIPANTE INT,
    NOME VARCHAR(150),
    PAIS_ORIGEM VARCHAR(100)
)
""")

participantes = [
    (1, 'Maria', 'Brasil'),
    (2, 'Luana', 'Portugal'),
    (3, 'Camila', 'Brasil'),
    (4, 'Luiza', 'Argentina'),
    (5, 'Silvia', 'Colombia'),
    (6, 'Paola', 'Brasil'),
    (7, 'Vitoria', 'Mexico'),
    (8, 'Caroline', 'Argentina'),
    (9, 'Marta', 'Portugal'),
    (10, 'Ana', 'Brasil')
]

cursor_bootcamp.executemany(
    "INSERT INTO PARTICIPANTES VALUES (?, ?, ?)",
    participantes
)

conn_bootcamp.commit()
conn_bootcamp.close()

print("Banco de Dados e tabela criados com sucesso!")

# ========= 1.4 - Data Warehouse =========

conn_datawarehouse = sqlite3.connect("data_warehouse.db")
conn_datawarehouse.close()

print("Data Warehouse criado com sucesso!")

# ========= 2.1 - Logger =========

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    filename="pipeline.log",
    filemode="w",
)
logger = logging.getLogger()
print("Logging configurado com sucesso!")

# ========= 2.2 - Extração CSV =========

def extrair_dados_kaggle(caminho_arquivo):
    try:
        logger.info(f"Inicando a extração do arquivo: {caminho_arquivo}")
        df = pd.read_csv(caminho_arquivo)
        logger.info(f"Extração do CSV concluída. {len(df)} linhas lidas e {len(df.columns)} colunas.")
        logger.info(f"Tipos de dados das colunas: \n{df.dtypes}")
        return df
    except FileNotFoundError:
        logger.error(f"Arquivo não encontrado: {caminho_arquivo}")
        return None
    except Exception as e:
        logger.error(f"Ocorreu um erro inesperado ao extrair os dados: {e}")
        return None

df_kaggle = extrair_dados_kaggle(
    r"C:\Users\ingri\Documents\BI\Semana 15\kaggle_survey_2022_mulheres_dados.csv"
)

# ========= 2.3 - Extração SQL =========

def extrair_dados_sql(caminho_banco):
    conexao = None
    try:
        logger.info(f"Inicando a extração do banco de dados: {caminho_banco}")
        conexao = sqlite3.connect(caminho_banco)
        query = "SELECT * FROM PARTICIPANTES"
        df = pd.read_sql_query(query, conexao)
        logger.info("Extração do banco de dados concluída com sucesso!")
        return df
    except sqlite3.Error:
        logger.error("Ocorreu um erro ao conectar ao banco de dados")
        return None
    except Exception as e:
        logger.error(f"Ocorreu um erro inesperado ao extrair os dados: {e}")
        return None
    finally:
        if conexao:
            conexao.close()
            logger.info("Conexão com o banco de dados fechada.")

df_participantes = extrair_dados_sql("bootcampBI.db")

# ========= 2.4 - Carga =========

def carregar_dados(df, nome_tabela, caminho_dw):
    if df is None:
        logger.warning("DataFrame vazio. Nada a ser carregado.")
        return

    conexao_dw = None
    try:
        logger.info(f"Iniciando o carregamento dos dados na tabela: {nome_tabela}")
        conexao_dw = sqlite3.connect(caminho_dw)
        df.to_sql(nome_tabela, conexao_dw, if_exists="replace", index=False)
        logger.info(f"Carga para a tabela '{nome_tabela}' concluída com sucesso!")
    except Exception as e:
        logger.error(f"Ocorreu um erro ao carregar os dados para a tabela '{nome_tabela}': {e}")
    finally:
        if conexao_dw:
            conexao_dw.close()
            logger.info("Conexão com o Data Warehouse fechada.")

carregar_dados(df_kaggle, "kaggle_survey_2022_mulheres_dados", "data_warehouse.db")
carregar_dados(df_participantes, "participantes", "data_warehouse.db")

# ==========================================
# 3. Extração e Carregamento - API e JSON
# ==========================================

def extrair_dados_paises_api(url_api):
    try:
        logger.info(f"Iniciando a requisição à API: {url_api}")
        resposta = requests.get(url_api)
        if resposta.status_code == 200:
            dados_json = resposta.json()
            logger.info(f"Dados da API extraídos com sucesso. {len(dados_json)} registros de países.")
            return dados_json
        else:
            logger.error(f"Falha na requisição à API. Código de status: {resposta.status_code}")
            return None
    except Exception:
        logger.error("Ocorreu um erro inesperado ao extrair os dados da API")
        return None

url_paises = "https://restcountries.com/v3.1/all?fields=name,cca3,region"
dados_paises = extrair_dados_paises_api(url_paises)
df_paises = pd.json_normalize(dados_paises) if dados_paises is not None else None

def extrair_categorias_habilidades_json(caminho_arquivo):
    try:
        logger.info(f"Iniciando a extração do arquivo JSON: {caminho_arquivo}")
        with open(caminho_arquivo, "r", encoding="utf-8") as arquivo:
            dados_json = json.load(arquivo)
        logger.info("Extração do JSON concluída com sucesso!")
        return dados_json
    except FileNotFoundError:
        logger.error(f"Arquivo não encontrado: {caminho_arquivo}")
        return None
    except Exception as e:
        logger.error(f"Ocorreu um erro inesperado ao extrair os dados: {e}")
        return None

dados_habilidades = extrair_categorias_habilidades_json(
    r"C:\Users\ingri\Documents\BI\Semana 15\habilidades_categorias.json"
)

def transformar_json_em_df(dados_json):
    if dados_json is None:
        logger.warning("Dados JSON vazios. Nada a ser transformado")
        return None

    lista_categorias = []
    for categoria, habilidades in dados_json.items():
        for habilidade in habilidades:
            lista_categorias.append({"CATEGORIA": categoria, "HABILIDADE": habilidade})

    df_categorias = pd.DataFrame(lista_categorias)
    logger.info("Transformação do JSON em DataFrame concluída com sucesso!")
    return df_categorias

df_categorias = transformar_json_em_df(dados_habilidades)

carregar_dados(df_paises, "paises", "data_warehouse.db")
carregar_dados(df_categorias, "categorias", "data_warehouse.db")

# ==========================
# 4. Configuração do dbt
# ==========================

profiles_config = {
    "pipeline_mulheres_na_tecnologia": {
        "target": "dev",
        "outputs": {
            "dev": {
                "type": "sqlite",
                "threads": 1,
                "schemas_and_paths": {
                    "main": "../data_warehouse.db"
                },
                "database": "../data_warehouse.db",
                "schema": "main",
                "schema_directory": ".",
            }
        },
    }
}

dbt_profile_dir = os.path.expanduser("~/.dbt/")
os.makedirs(dbt_profile_dir, exist_ok=True)
profiles_path = os.path.join(dbt_profile_dir, "profiles.yml")

with open(profiles_path, "w", encoding="utf-8") as f:
    yaml.dump(profiles_config, f)

# ==========================================
# 7. Orquestração com Prefect
# ==========================================

@task(retries=3, retry_delay_seconds=5)
def t_extrair_dados_kaggle(caminho_arquivo):
    return extrair_dados_kaggle(caminho_arquivo)

@task
def t_extrair_dados_sql(caminho_banco):
    return extrair_dados_sql(caminho_banco)

@task
def t_extrair_dados_paises_api(url_api):
    return extrair_dados_paises_api(url_api)

@task
def t_extrair_categorias_habilidades_json(caminho_arquivo):
    return extrair_categorias_habilidades_json(caminho_arquivo)

@task
def t_carregar_dados(df, nome_tabela, caminho_dw):
    carregar_dados(df, nome_tabela, caminho_dw)

@task
def executar_dbt_run():
    logger = get_run_logger()
    logger.info("Iniciando 'dbt run'...")
    try:
        # certifique-se de ter rodado: dbt init pipeline_mulheres_na_tecnologia
        subprocess.run(
            ["dbt", "run"],
            check=True,
            cwd=r"C:\Users\ingri\Documents\BI\Semana 15\pipeline_mulheres_na_tecnologia",
        )
        logger.info("'dbt run' concluído com sucesso.")
    except subprocess.CalledProcessError as e:
        logger.error(f"Falha no 'dbt run': {e}")
        raise

@flow(name="Pipeline ETL - Mulheres na Tecnologia")
def pipeline_principal():
    logger = get_run_logger()
    logger.info("### INICIANDO O PIPELINE DE ETL ###")

    df_kaggle_f = t_extrair_dados_kaggle(
        r"C:\Users\ingri\Documents\BI\Semana 15\kaggle_survey_2022_mulheres_dados.csv"
    )
    df_participantes_f = t_extrair_dados_sql("bootcampBI.db")
    dados_paises_f = t_extrair_dados_paises_api(url_paises)
    dados_habilidades_f = t_extrair_categorias_habilidades_json(
        r"C:\Users\ingri\Documents\BI\Semana 15\habilidades_categorias.json"
    )

    df_paises_f = pd.json_normalize(dados_paises_f) if dados_paises_f is not None else None
    df_categorias_f = transformar_json_em_df(dados_habilidades_f)

    carga_kaggle = t_carregar_dados(df_kaggle_f, "kaggle_survey_2022_mulheres_dados", "data_warehouse.db")
    carga_participantes = t_carregar_dados(df_participantes_f, "participantes", "data_warehouse.db")
    carga_paises = t_carregar_dados(df_paises_f, "paises", "data_warehouse.db")
    carga_habilidades = t_carregar_dados(df_categorias_f, "categorias", "data_warehouse.db")

    executar_dbt_run(wait_for=[carga_kaggle, carga_participantes, carga_paises, carga_habilidades])

    logger.info("### PIPELINE DE ETL CONCLUÍDO COM SUCESSO ###")

if __name__ == "__main__":
    pipeline_principal()
